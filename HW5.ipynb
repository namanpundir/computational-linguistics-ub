{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pCRaCjtDiua"
      },
      "source": [
        "# HW5: Building and evaluating a part-of-speech tagger\n",
        "\n",
        "In this assignment, we are interested in what factors affect how well a trained part-of-speech tagger will do on unseen data. The homework will assess your ability to manipulate data (the output of neural language models) and your ability to discuss the results from each of the classifiers that you train. You will train four classifiers that will focus on different ways of looking at the data.\n",
        "\n",
        "We specifically want you to assess the performance of a deep neural language model ([RoBERTa; Liu et al., 2019](https://arxiv.org/abs/1907.11692)) that is readibly available in the `huggingface` package. We would like you to compare and contrast the performance of classifiers trained on representations obtained from a lower layer of RoBERTa with classifiers trained on a higher layer from RoBERTa. This will allow you to see how different parts of a neural model's architecture do or do not encode the same kind of information.\n",
        "\n",
        "We also want to see the effect of how close the training data is to the test data. This is like a real-world scenario, where we often train on the recent past to predict the present. So, we want to get the right part-of-speech tags for this year's abstracts (2021) either from classifiers trained on abstract part-of-speech tags from the immediately preceding year (2020) or any year prior to 2020. Think about how language and science can change while you are building these classifiers. What does it mean if 2020 and pre-2020 influence performance on tagging the 2021 dataset?\n",
        "\n",
        "### Before you start: What are part-of-speech tags?\n",
        "\n",
        "Part of speech tags are labels we assign to words depending on what kind of syntactic role they play in a sentence. While we have not studied part-of-speech tags yet in class, they have come up when we have talked about nouns, verbs, modifiers, etc. Building a good classifier that can do part-of-speech tagging can help us better understand things like the syntactic structure of a sentence. In order to understand the meaning of a chunk of language, we need to know what kind of \"role\" each word is playing.\n",
        "\n",
        "There are lots of resources for learning lots about part of speech tags. For this assignment, we will work with the most basic of categories: \"Universal\" labels. These categories are designed to work for as many languages as possible. We are trying to predict the following categories in context to the best of our ability:\n",
        "\n",
        "    VERB - verbs (all tenses and modes)\n",
        "    NOUN - nouns (common and proper)\n",
        "    PRON - pronouns\n",
        "    ADJ - adjectives\n",
        "    ADV - adverbs\n",
        "    ADP - adpositions (prepositions and postpositions)\n",
        "    CONJ - conjunctions\n",
        "    DET - determiners\n",
        "    NUM - cardinal numbers\n",
        "    PRT - particles or other function words\n",
        "    X - other: foreign words, typos, abbreviations\n",
        "    . - punctuation\n",
        "\n",
        "So, our classifiers will try to learn what makes something an \"adjective\", what makes something a \"noun\", and so on.\n",
        "\n",
        "## Warning: This assignment will probably take a long time!!\n",
        "## Lots of moving parts and many computations are very slow.\n",
        "## Please heed the advice below:\n",
        "\n",
        "* ### We recommend that you prototype on very small subsets of the data (e.g., `train_2020_only[0:5]` and `test_2021[0:5]`)\n",
        "* ### Only once you are ready to submit your assignment and start writing up your results should you run through the whole dataset.\n",
        "* ### Running RoBERTa and training your classifier can easily take half an hour or more to run depending on the efficiency of your implementation. When you have finished prototyping, expect for this to take a full 3-4 hours, just in case.\n",
        "\n",
        "## DO NOT start until the last minute. It will only lead to avoidable suffering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf0Zg3C6R0iO"
      },
      "source": [
        "# Q1: Installing prerequisites (2 points)\n",
        "\n",
        "In order to do this assignment, you need to install the `transformers` package from `huggingface`. Do that in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvrD2GRyukeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOvM2GhGRW6Z"
      },
      "source": [
        "# Q2: Imports (1 point)\n",
        "\n",
        "Put all of the imports you will use here. Also include the neural language model and tokenizer that you will use. We will be using the RoBERTa models; for examples, refer to lecture notebooks. Keep in the `model.eval()` code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqAwrluGRa07"
      },
      "source": [
        "# your imports go here\n",
        "\n",
        "\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgUe790dAbGc"
      },
      "source": [
        "#Q3: Data preprocessing (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA3sAwSuRD1j"
      },
      "source": [
        "## Q3A: Loading in the three datasets (3 points)\n",
        "\n",
        "For this assignment, we are going to use best practices in  machine learning and split our training data and our test data apart. We have two training datasets for you, which we described above. The 2020-only dataset is called `train_2020-only.json` and the pre-2020 dataset is called `train_pre-2020.json`. Each line correponds to one `json` object. Load in each of these training datasets as `train_2020_only` and `train_pre2020` respectively using our familiar friend `json.loads`, reading in the data line by line.\n",
        "\n",
        "Our test data is stored in the file `test_2021.json`. It is structured exactly the same way as the training files, but when you load it in, name it `test_2021`.\n",
        "\n",
        "All of the datasets are stored in the `data/` subdirectory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTEpHZzyRbnL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8ElXY_40BZ6"
      },
      "source": [
        "## Q3B: Preview data (1 point)\n",
        "\n",
        "Print out the first two entries of `train_2020_only`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KZ08zlz0Eck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeu9UjV20QIW"
      },
      "source": [
        "## Q3C: What are each of the lines? (2 points)\n",
        "\n",
        "What kind of data structure is it? What are the elements?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n7-6Vg50Vfk"
      },
      "source": [
        "<font color=\"red\">----Your answer goes here----</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHJyTlK_R-UC"
      },
      "source": [
        "# Q4: Creating embeddings for each utterance (25 points) for your training data and producing four models\n",
        "\n",
        "For hints, check out the `natural_language_inference.ipynb` functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyi4JkZV0hqY"
      },
      "source": [
        "Use the below function to take a single sentence and turn it into an embedding that we can use for our classifiers. This model will automatically ignore all non-initial morphemes so you do not have to worry about how RoBERTa handles word pieces.\n",
        "\n",
        "Pay attention to the `# note` in the below for a clue to a later question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqHAAHVe05aY"
      },
      "source": [
        "def embed_words_roberta(single_data_entry, model, tokenizer):\n",
        "  words_only = [x[0] for x in single_data_entry] # note\n",
        "  tokenized = tokenizer(words_only, return_tensors='pt',\n",
        "                        is_split_into_words=True)\n",
        "  embedded = model(**tokenized)\n",
        "  embeddings = embedded['hidden_states']\n",
        "  token_strings = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0].tolist())\n",
        "  dimensions_to_keep = [i for i, x in enumerate(token_strings)\n",
        "                        if x.startswith(\"Ä \") or i==1]\n",
        "  subsetted_embeddings = [x[:, dimensions_to_keep].detach().numpy()\n",
        "                          for x in embeddings]\n",
        "  return subsetted_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF_q8JPV5ihG"
      },
      "source": [
        "## Q4A: Extract the embeddings at a specific layer (2 points)\n",
        "\n",
        "Please print out the 7th layer from the output of `embed_words_roberta(train_2020_only[0])`.\n",
        "\n",
        "Then print out the 3rd layer.\n",
        "\n",
        "Remember Python indexing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJuqm7Tj5d6z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsuM-UT33v3"
      },
      "source": [
        "##Q4B: Complete the function `process_training_dataset` (7 points)\n",
        "\n",
        "Your code below should be able to take a given dataset that you loaded in above and produce word embeddings for each word. Then, these word embeddings will be used to train a classifier. All you need to do is make sure your Xs and ys are shaped right and you should be good to go.\n",
        "\n",
        "The function `process_training_dataset` critically must take in:\n",
        "\n",
        "* A dataset (e.g., any of the above)\n",
        "* A neural language model\n",
        "* A tokenizer that the neural language model can work with\n",
        "* A specific layer number that we subset to when building our training data\n",
        "\n",
        "And it will return:\n",
        "* A trained classifier that can assign part-of-speech tags given word embeddings\n",
        "\n",
        "**Note**: Pay attention to your answer in the previous question so you can better extract the right word embeddings for all of your classifiers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69lnZN3RzIvD"
      },
      "source": [
        "def process_training_dataset(dataset, neural_model, neural_tokenizer, layer_number):\n",
        "  # define your Xs and ys (embeddings and POS tags)\n",
        "  Xs, ys = [], []\n",
        "  # loop over every sentence in the dataset\n",
        "    # extract POS tags for that sentence\n",
        "    # combine ys with the POS tags\n",
        "\n",
        "    # extract embeddings for that sentence\n",
        "\n",
        "    # get embeddings at a specific layer\n",
        "\n",
        "  Xs = np.vstack(Xs)\n",
        "  classifier = LogisticRegression(max_iter=1000)\n",
        "  classifier.fit(Xs, ys)\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np9O4uLoyDZt"
      },
      "source": [
        "##Q4C: Train your model @ layer 0 on the 2020-only data (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSIuQYCPNVI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2am1nWNJyGXf"
      },
      "source": [
        "## Q4D: Model @ layer 0, on pre-2020 data (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohn8fdhNPLSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38d-8wOVyIs5"
      },
      "source": [
        "## Q4E: Model @ layer 10, on 2020-only data (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e68D2clLPLt6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzwTrjEOyKVm"
      },
      "source": [
        "## Q4F: Model @ layer 10, on pre-2020 data (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoV94YzePMOM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFfDgejSCnG"
      },
      "source": [
        "# Q5: Evaluate and compare all models (18 points)\n",
        "\n",
        "For this question, we would like you to compare each of the models to each other along several dimensions. The four models cross the layer within the model (0 or 10) and whether the model is trained on older or newer data (pre-2020 or 2020 data). In order to compare the models, you need to get each of your models to generate **predicted** labels for each of the test items. First, you will need to **construct your test dataset** and then evaluate each model along the following dimensions using the following functions:\n",
        "\n",
        "* Precision (`sklearn.metrics.precision_score`)\n",
        "* Recall (`sklearn.metrics.recall_score`)\n",
        "* F1 (`sklearn.metrics.f1_score`)\n",
        "\n",
        "Then, you will be asked to fill in the performance of each of these four models in the form of a table. You will find instances of this around some of the previous lectures (e.g., the natural language inference and evaluation lecture notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJVtas9uQ-rd"
      },
      "source": [
        "## Q5A: Test data processing (3 points)\n",
        "\n",
        "In order for us to assess the ability for our models above to do well, we have to also process our test data. The way `scikit-learn` expects to produce predictions is very simple. When we _train_ a `LogisticRegression` model, we give the model as input some set of $X$ values (e.g., a matrix of word embeddings). The model we train tries to optimize the fit between $X$ and the $y$ values we give -- such as the labels associated with part-of-speech tags. Getting _predictions_ from our trained models is simply a matter of giving it new $X$ values -- from our test dataset.\n",
        "\n",
        "In order for this to work, we also have to process our test data to conform to the same structure as our training data. So, for this question, we would like you to make a function `process_test_dataset` that is just like the `process_train_dataset` but there is no need to train a model at the end at all. Instead, the function _only_ needs to return `Xs` (a matrix containing word embeddings) and `ys` (part-of-speech tags). The stub of what you need to do is below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9RfgiucZEex"
      },
      "source": [
        "def process_test_dataset():\n",
        "  Xs, ys = [], []\n",
        "  # implement the test version of process_train_dataset\n",
        "\n",
        "  return Xs, ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSBN-mGKSV26"
      },
      "source": [
        "## Q5B: Score all four models (12 points)\n",
        "\n",
        "Loop through each of your four models (output of last four notebook cells), print the precision, recall, and f1 scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPiGfp1yfvmG"
      },
      "source": [
        "# use precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw7VwboyPXwb"
      },
      "source": [
        "## Q5C: Free response (3 points)\n",
        "\n",
        "Using the outputs above, fill out a table showing performance across each of the 4 models, along all 3 measures. Describe in words how the models differ in their performance. Are there any patterns you notice that determine model performance? Were any of the results surprising to you? Why or why not? If the differences are small, can you think of a reason why we might not trust these results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIo3lBuJPfq4"
      },
      "source": [
        "<font color=\"red\">Your free response goes here.</font>\n",
        "\n",
        "<font color=\"red\">Your table goes here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkkRxQe3SEnb"
      },
      "source": [
        "# Bonus: Error analysis (6 points; 3 for code, 3 for free response)\n",
        "\n",
        "*   Take the best-performing model\n",
        "*   Construct a confusion matrix in any way that you would like, comparing the output of the best model on your test set and the true labels.\n",
        "*   What categories are most confusable? What linguistic reasons might that be the case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYcR1M6Fc37B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bypCHKcc-tqJ"
      },
      "source": [
        "<font color=\"red\">Your bonus question answer goes here.</font>"
      ]
    }
  ]
}